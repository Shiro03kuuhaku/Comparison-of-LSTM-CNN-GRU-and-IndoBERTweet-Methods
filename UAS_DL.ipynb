{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shiro03kuuhaku/Comparison-of-LSTM-CNN-GRU-and-IndoBERTweet-Methods/blob/main/UAS_DL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImxLWH0Fj5Vt"
      },
      "outputs": [],
      "source": [
        "%pip install tf-keras==2.11.0 tensorflow==2.11.0 --quiet\n",
        "import os\n",
        "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uFD69DeHj_n4"
      },
      "outputs": [],
      "source": [
        "# === 1. Import & Preprocessing ===\n",
        "import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns\n",
        "import tensorflow as tf\n",
        "import re, string\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from nltk.corpus import stopwords\n",
        "import nltk; nltk.download('stopwords')\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, GRU, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# === 2. Load Dataset ===\n",
        "df = pd.read_csv(\"PRDECT_ID.csv\")\n",
        "df.columns = df.columns.str.strip()\n",
        "df.rename(columns={'Customer Review': 'review', 'Emotion': 'emotion'}, inplace=True)\n",
        "df.dropna(subset=['review', 'emotion'], inplace=True)\n",
        "\n",
        "# === 3. Cleaning & Encoding ===\n",
        "stop_words = set(stopwords.words('indonesian'))\n",
        "def clean_text(t):\n",
        "    t = \"\".join(c for c in t.lower() if c.isalpha() or c.isspace())\n",
        "    return \" \".join(w for w in t.split() if w not in stop_words)\n",
        "\n",
        "df['clean_review'] = df['review'].apply(clean_text)\n",
        "label_encoder = LabelEncoder()\n",
        "df['label'] = label_encoder.fit_transform(df['emotion'])\n",
        "num_classes = len(label_encoder.classes_)\n",
        "\n",
        "# === 4. Tokenizing LSTM/GRU/CNN ===\n",
        "tokenizer = Tokenizer(oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(df['clean_review'])\n",
        "X_seq = tokenizer.texts_to_sequences(df['clean_review'])\n",
        "X_pad = pad_sequences(X_seq, maxlen=100, padding='post')\n",
        "y = df['label'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pad, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# === 5. Build & Train LSTM ===\n",
        "model_lstm = Sequential([\n",
        "    Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=128, input_length=100),\n",
        "    LSTM(64),\n",
        "    Dropout(0.5),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "hist_lstm = model_lstm.fit(X_train, y_train, validation_split=0.1, epochs=3, batch_size=32)\n",
        "model_lstm.save(\"model_lstm.h5\")\n",
        "\n",
        "# === 6. Build & Train GRU ===\n",
        "model_gru = Sequential([\n",
        "    Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=128, input_length=100),\n",
        "    GRU(64),\n",
        "    Dropout(0.5),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "model_gru.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "hist_gru = model_gru.fit(X_train, y_train, validation_split=0.1, epochs=3, batch_size=32)\n",
        "model_gru.save(\"model_gru.h5\")\n",
        "\n",
        "# === 7. Build & Train CNN ===\n",
        "model_cnn = Sequential([\n",
        "    Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=128, input_length=100),\n",
        "    Conv1D(128, 5, activation='relu'),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "model_cnn.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "hist_cnn = model_cnn.fit(X_train, y_train, validation_split=0.1, epochs=3, batch_size=32)\n",
        "model_cnn.save(\"model_cnn.h5\")\n",
        "\n",
        "# === 8. IndoBERTweet ===\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(\"cahya/bert-base-indonesian-522M\")\n",
        "bert_enc_train = bert_tokenizer(list(df['clean_review']), padding=True, truncation=True, max_length=128, return_tensors='tf')\n",
        "dataset = tf.data.Dataset.from_tensor_slices(({\n",
        "    'input_ids': bert_enc_train['input_ids'],\n",
        "    'attention_mask': bert_enc_train['attention_mask']\n",
        "}, df['label'].values)).batch(16)\n",
        "\n",
        "bert_model = TFAutoModelForSequenceClassification.from_pretrained(\"cahya/bert-base-indonesian-522M\", num_labels=num_classes)\n",
        "bert_model.compile(optimizer=Adam(learning_rate=2e-5),\n",
        "                   loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                   metrics=['accuracy'])\n",
        "bert_model.fit(dataset, epochs=3)\n",
        "bert_model.save_pretrained(\"bert_model\")\n",
        "\n",
        "# === 9. Evaluation Function ===\n",
        "def evaluate_model(model, X, y_true, name, is_bert=False):\n",
        "    if is_bert:\n",
        "        preds = model.predict(X)['logits']\n",
        "        y_pred = np.argmax(preds, axis=1)\n",
        "    else:\n",
        "        preds = model.predict(X)\n",
        "        y_pred = np.argmax(preds, axis=1)\n",
        "    print(f\"\\n--- {name} ---\")\n",
        "    print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    sns.heatmap(cm, annot=True, fmt='d', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
        "    plt.title(f'Confusion Matrix: {name}'); plt.xlabel('Pred'); plt.ylabel('True'); plt.show()\n",
        "\n",
        "# === 10. Evaluation ===\n",
        "evaluate_model(model_lstm, X_test, y_test, \"LSTM\")\n",
        "evaluate_model(model_gru, X_test, y_test, \"GRU\")\n",
        "evaluate_model(model_cnn, X_test, y_test, \"CNN\")\n",
        "\n",
        "# === 11. Evaluate BERT ===\n",
        "bert_enc_test = bert_tokenizer(list(df['clean_review']), padding=True, truncation=True, max_length=128, return_tensors='tf')\n",
        "bert_test_dataset = {\n",
        "    'input_ids': bert_enc_test['input_ids'],\n",
        "    'attention_mask': bert_enc_test['attention_mask']\n",
        "}\n",
        "evaluate_model(bert_model, bert_test_dataset, df['label'].values, \"BERT\", is_bert=True)\n",
        "\n",
        "# === 12. Plot Accuracy & Loss ===\n",
        "def plot_history(history, title):\n",
        "    plt.figure(figsize=(10,4))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(history.history['accuracy'], label='train')\n",
        "    plt.plot(history.history['val_accuracy'], label='val')\n",
        "    plt.title(f'{title} Accuracy'); plt.legend()\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(history.history['loss'], label='train')\n",
        "    plt.plot(history.history['val_loss'], label='val')\n",
        "    plt.title(f'{title} Loss'); plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_history(hist_lstm, \"LSTM\")\n",
        "plot_history(hist_gru, \"GRU\")\n",
        "plot_history(hist_cnn, \"CNN\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOM+/LRRzUz+Af0r3ZnJV8T",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}